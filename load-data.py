#! /usr/bin/env python3
# Load CVE data into CouchDB database.
# - CVE data (vers 1.1) from NVD.
# -

import requests, gzip, json, argparse, datetime, sys, glob
from time2relax import CouchDB # For bulk loading https://pypi.org/project/time2relax/
from pathlib import Path

parser = argparse.ArgumentParser(
    prog='tba',
    description='Load CVE data into CouchDB.')

parser.add_argument('--start', '-s', help='Start year (inclusive)', default=1999, type=int)
parser.add_argument('--end', '-e', help='End year (inclusive)',  default=datetime.datetime.now().year, type=int)

parser.add_argument('--dbport',       help='Local CouchDB port number', default='5984', type=int)
parser.add_argument('--dbuser', '-u', help='CouchDB admin username', default='admin')
parser.add_argument('--dbpass', '-p', help='CouchDB admin password', default='password')
parser.add_argument('--dbname', '-n', help='CouchDB database name', default='cve')

parser.add_argument('--dir', '-d',  help='CVE list V5 directory')

global args
args = vars(parser.parse_args())

if args['dir'][0] == '~':
    args['dir'] = Path(args['dir']).expanduser()


if args['dir'][0] == '.':
    args['dir'] = Path(args['dir']).resolve()


def make_batch(l, s):
    """
    Yields batch of size s from list l
    """
    for i in range(0, len(l), s):
        yield l[i : i + s]


def bulk_load_json(json_content, object_name, batch_size: int = 100):
    """
    json_content:
    object_name: JSON key
    """
    json_content_object = json.loads(json_content)

    for batch in make_batch(json_content_object[object_name], batch_size):
        db.bulk_docs(batch)
        # TODO Check success


def load_nvd(db: CouchDB, start: int, end: int) -> None:
    """
    Download (vers) version json.gz files from NVD for year range (start) to (end)
    and load into CouchDB (db).
    """
    vers = '1.1'
    nvd_url = "https://nvd.nist.gov/feeds/json/cve"

    for year in range(start, end+1):
        # TODO build url properly
        url_for_year = f'{nvd_url}/{vers}/nvdcve-{vers}-{year}.json.gz'

        print(year)

        r = requests.get(url_for_year)
        # TODO Handle errors: timeout

        assert r.status_code == 200, f"Bad response code {r.status_code}"
        assert r.headers['content-type'] == 'application/x-gzip', 'Bad content type'

        json_content = gzip.decompress(r.content)
        bulk_load_json(json_content, 'CVE_Items')



def load_cvep (db: CouchDB, dir: str, start: int, end: int) -> None:
    """
    Load CVE Project V5 JSON files
    """
    list_of_json_files = list()

    for year in range(start, end+1):
        p = Path(dir, str(year), '*', '*.json').as_posix()
        list_of_json_files += glob.glob(p, recursive=True)

    print(f"{len(list_of_json_files)} JSON files")

    # Get batch of files
    for batch in make_batch(list_of_json_files, 10):
        print(batch)
        sys.exit(1)




if __name__ == '__main__':
    # TODO Delete DB if already present?
    server = f"http://{args['dbuser']}:{args['dbpass']}@localhost:{args['dbport']}"

    sd = f"{server}/{args['dbname']}"
    db = CouchDB(sd, create_db=True)

    print('Load data into CouchDB') # TODO add some info about which server

    #load_nvd(db = db, start = args['start'], end = args['end'])

    load_cvep(db = db, dir = args['dir'], start = args['start'], end = args['end'])

    # TODO Query and count number of docs
    print('Done')
